<html lang="en-GB">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance</title>
    <meta name="description"
        content="We've presented VideoGameQA-Bench, a comprehensive benchmark for evaluating Vision-Language Models in video game QA tasks.">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <meta name="robots" content="all">
    <meta content="en_EN" property="og:locale">
    <meta content="website" property="og:type">
    <meta content="https://shikun.io/projects/clarity" property="og:url">
    <meta content="VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance"
        property="og:title">
    <meta content="A comprehensive benchmark for evaluating Vision-Language Models in video game QA tasks."
        property="og:description">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@your_twitter_id">
    <meta name="twitter:description"
        content="VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance. A comprehensive benchmark for evaluating Vision-Language Models in video game QA tasks.">
    <meta name="twitter:image:src" content="assets/figures/25AE7184-F49E-4322-98DE-4B558D88BC8B.png">

    <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
    <link rel="stylesheet" type="text/css" media="all" href="clarity/clarity.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
    <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css" />
    <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
    <script src="assets/scripts/navbar.js"></script> <!-- Comment to remove table of content. -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": {
              scale: 95,
              fonts: ["Gyre-Pagella"],
              imageFont: null,
              undefinedFamily: "'Arial Unicode MS', cmbright"
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
          });
    </script>
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
</head>

<body>
    <!-- Title Page -->
    <!-- Dark Theme Example: Change the background colour dark and change the following div "blog-title" into "blog-title white". -->
    <div class="container blog" id="first-content" style="background-color: #E0E4E6;">
        <!-- If you don't have a project cover: Change "blog-title" into "blog-title no-cover"  -->
        <div class="blog-title">
            <div class="blog-intro">
                <div>
                    <h1 class="title">VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality
                        Assurance</h1>
                    <p class="author">Mohammad Reza Taesiri, Abhijay Ghildyal, Saman Zadtootaghaj, Nabajeet Barman,
                        Cor-Paul Bezemer</p>
                    <p class="abstract">
                        With video games now generating the highest revenues in the entertainment industry, optimizing
                        game development workflows has become essential for the sector's sustained growth. Recent
                        advancements in Vision-Language Models (VLMs) offer considerable potential to automate and
                        enhance various aspects of game development, particularly Quality Assurance (QA), which remains
                        one of the industry's most labor-intensive processes with limited automation options. To
                        accurately evaluate the performance of VLMs in video game QA tasks and determine their
                        effectiveness in handling real-world scenarios, there is a clear need for standardized
                        benchmarks, as existing benchmarks are insufficient to address the specific requirements of this
                        domain. To bridge this gap, we introduce VideoGameQA-Bench, a comprehensive benchmark that
                        covers a wide array of game QA activities, including visual unit testing, visual regression
                        testing, needle-in-a-haystack tasks, glitch detection, and bug report generation for both images
                        and videos of various games.
                    </p>

                </div>

                <div class="info">
                    <div>
                        <a href="https://arxiv.org" class="button icon"
                            style="background-color: rgba(255, 255, 255, 0.2)"> Paper <i
                                class="fa-solid fa-book-open"></i></a> &nbsp;&nbsp;
                        <a href="https://github.com" class="button icon"
                            style="background-color: rgba(255, 255, 255, 0.2)">Artifacts <i
                                class="fa-solid fa-file-code"></i></a>
                        &nbsp;&nbsp;
                        <a href="https://huggingface.co/datasets/taesiri/VideoGameQA-Bench" class="button icon"
                            style="background-color: rgba(255, 255, 255, 0.2)">Dataset <i
                                class="fa-solid fa-database"></i></a>
                    </div>
                </div>
            </div>

            <div class="blog-cover">
                <img class="foreground" src="assets/figures/25AE7184-F49E-4322-98DE-4B558D88BC8B.png">
                <img class="background" src="assets/figures/25AE7184-F49E-4322-98DE-4B558D88BC8B.png">
            </div>
        </div>
    </div>


    <div class="container blog main first" id="blog-main">
        <h1>
            Introduction
        </h1>
        <p>
            The global video game industry continues to expand rapidly, with its market value projected to reach $257
            billion by
            2028. Alongside this substantial growth, the process of developing high-quality video games remains
            inherently
            complex and demanding. A critical challenge within game development is to ensure visual quality and
            consistency
            through a rigorous visual testing and quality assurance (QA) process. Automation of visual QA tasks remains
            particularly challenging, and currently, most visual QA relies heavily on manual inspection, making the
            process
            time-consuming, costly, labor-intensive, and prone to human error.
        </p>

        <br>

        <p>
            The visual QA process for video games can generally be abstracted into three main types of tasks:
        </p>
        <br>
        <li>
            <strong>Verifying scene integrity</strong> by comparing the visual representation of scenes against
            intended
            configurations and known reference states, such as an oracle or previously rendered versions of the same
            scenes.
        </li>
        <li>
            <strong>Detecting glitches</strong> through open-ended explorationâ€”these glitches are unintended
            gameplay or
            visual artifacts without specific reference points, requiring testers to rely on common sense and
            general
            knowledge for detection.
        </li>
        <li>
            <strong>Systematically reporting and documenting</strong> all identified glitches, ensuring developers
            receive
            clear and actionable information to address problems effectively during game development.
        </li>
        </ol>

        <br>

        <p>
            Recent advancements in vision-language models (VLMs) present promising opportunities to automate and
            significantly
            enhance the efficiency of video game QA. However, progress in applying VLMs to game QA has been limited by
            the lack
            of standardized benchmarks. Current multimodal benchmarks tend to focus heavily on complex mathematical or
            textual
            reasoning tasks, overlooking essential visual comprehension tasks fundamental to video game QA. Similarly,
            existing
            game-specific benchmarks often represent only narrow aspects of QA tasks, thus inadequately evaluating and
            tracking
            VLM performance across diverse QA scenarios.
        </p>
    </div>


    <div class="container blog main" id="blog-main">
        <h1>
            Our Contributions
        </h1>
        <p>
            In this paper, we introduce <strong>GameQA-Benchmark</strong>, a benchmark designed to fill the gap in
            evaluating
            VLMs for video game QA. Our key findings and contributions are as follows:
        </p>
        <br>

        <ol>
            <li>
                We present <strong>VideoGameQA-Benchmark</strong> featuring 9 distinct tasks and a large set of
                questions
                designed
                considering real-world video game development scenarios, such as visual unit testing, regression
                testing, UI
                validation, video needle-in-a-haystack, and glitch detection.
            </li>
            <li>
                While VLMs show promising performance on various multimodal benchmarks and can function as OCR systems,
                they
                perform poorly at detecting fine details required for accurate scene understanding and parsing complex
                UI
                elements.
            </li>
            <li>
                Frontier VLMs show good performance on the glitch detection task using images (up to 82.8%) and videos
                (up to
                78.1%); however, all struggle when it comes to glitches related to body configuration, intricate object
                clipping, and common-sense reasoning.
            </li>
            <li>
                Visual regression testing remains one of the most challenging tasks for VLMs.
            </li>
            <li>
                Locating specific glitch moments in videos remains a challenge, both in detecting and accurately
                pinpointing the
                glitch.
            </li>
            <li>
                Frontier VLMs can generate useful bug reports for up to 50% of real-world glitches, providing accurate
                and
                descriptive summaries of the glitches.
            </li>
        </ol>
    </div>


    <div class="container blog main">
        <h1>
            VideoGameQA-Bench
        </h1>
        <p>
            We designed <strong>VideoGameQA-Benchmark</strong>'s tasks by simulating realistic QA scenarios encountered
            during actual video game development. However, to make the benchmark more relevant for future QA automation
            tasks, we also included tasks that may challenge current software engineering practices while also remaining
            highly
            relevant. The table below provides an overview of the contents of each task in the benchmark.
            In summary, <strong>VideoGameQA-Benchmark</strong> contains 2,236 image-based samples and 1,200
            video-based
            samples sourced from more than 800 games and 9 synthetic game scenes.
        </p>

    </div>





    <div class="container blog main">
        <h1>
            Image-based Tasks
        </h1>
        <h2>Visual Unit Testing</h2>
        <p>
            Visual unit tests verify visual attributes including presence, placement, positioning, colors, conditions,
            and
            other relevant properties of various image elements.
        </p>
    </div>


    <div class="container blog main gray">
        <div class="slide-menu">
            <ul class="dots" id="slide-menu">
                <li class="dot active"></li>
                <li class="dot"></li>
                <li class="dot"></li>
            </ul>
        </div>


        <div class="slide-content" , style="display: block;">
            <img src="assets/samples/visual_unit_test/eyes.jpg" style="width: 100%;">


            <div style="margin-top: 1em; font-size: small;">
                <p>How many of Spider-Manâ€™s and Black Catâ€™s eye areas, including those covered by their masks, are
                    visible in the
                    image?
                </p>
                <br>
                <p>Provide your answer in the following JSON format:</p>
                <br>
                <pre><code class="javascript">{
    "spiderman_eyes_visible": 0,
    "black_cat_eyes_visible": 0
}</code></pre>
            </div>

        </div>

        <div class="slide-content" , style="display: none;">
            <img src="assets/samples/visual_unit_test/bird.jpg" style="width: 100%;">


            <div style="margin-top: 1em; font-size: small;">
                <p>Based on the image answer the following questions:</p>
                <br>

                <ol style="margin-left: 2em;">
                    <li>How many birds are visible inside the room (including inside the cage)?</li>
                    <li>How many birds are visible outside the window?</li>
                    <li>Is the birdcage door open?</li>
                    <li>What is the primary color of the bird inside the cage?</li>
                    <li>Is there a piece of wood leaning on the inside windowsill?</li>
                    <li>What is the main color of the blossoms seen outside the window?</li>
                    <li>Is the wallpaper on the left wall patterned?</li>
                    <li>Where is the grey bird positioned?</li>
                </ol>
                <br>

                <p>Provide your answer in the following JSON format:</p>
                <br>
                <pre><code class="javascript">{
    "birds_inside_count": 0, // Integer count
    "birds_outside_count": 0, // Integer count
    "birdcage_door_open": false, // true or false
    "bird_in_cage_color": "", // options: ["yellow", "grey", "blue", "brown"]
    "wood_on_sill_present": false, // true or false
    "blossom_color": "", // options: ["pink", "white", "yellow", "red"]
    "left_wallpaper_patterned": false, // true or false
    "grey_bird_location": "" // options: ["inside cage", "on windowsill", "outside window", "on floor"]
}
                </code></pre>
            </div>
        </div>

        <div class="slide-content" , style="display: none;">
            <img src="assets/samples/visual_unit_test/car.jpg" style="width: 100%;">


            <div style="margin-top: 1em; font-size: small;">
                <p>Based on the image answer the following questions:</p>

                <ol style="margin-left: 2em;">
                    <li>What is the primary color of the rally car?</li>
                    <li>Is the driver-side door of the car open or closed?</li>
                    <li>What number is displayed in large font on the car's door?</li>
                    <li>What brand name is visible on the yellow decal above the 'elf' logo on the car's side?</li>
                    <li>Is there a coiled orange air hose hanging from the ceiling on the left side?</li>
                    <li>What type of pattern is on the floor directly beneath the car?</li>
                    <li>Is there a screen or monitor mounted on the wall displaying graphs?</li>
                </ol>

                <p>Provide your answer in the following JSON format:</p>

                <pre><code class="javascript">{
    "car_primary_color": "", // options: ["light blue", "dark blue", "white", "red", "black"]
    "driver_door_state": "", // options: ["open", "closed"]
    "car_door_number": 0, // Integer value
    "yellow_decal_brand": "", // String value representing the text
    "coiled_hose_visible": false, // true or false
    "floor_pattern": "", // options: ["plain", "checkered", "tiled", "textured_metal"]
    "wall_monitor_visible": false // true or false
}
                </code></pre>
            </div>
        </div>


        <p class="caption">
            Sample from the Visual Unit Testing task.
        </p>
    </div>


    <div class="container blog main">
        <h2>UI Unit Testing</h2>
        <p>
            UI (visual) unit tests verify in-game UI elements such as menus, subtitles, heads-up displays (HUDs), and
            interface components like graphs and charts. We simulate the UI unit testing tasks by asking the
            vision-language
            model questions about game screenshots.
        </p>
    </div>

    <div class="container blog extra-large gray">
        <img src="clarity/images/prismer.png" alt="UI Unit Testing Sample">
        <p class="caption">
            Sample from the UI Unit Testing task.
        </p>
    </div>

    <div class="container blog main">
        <h2>Visual Regression Testing</h2>
        <p>
            Visual regression tests check for unintended visual changes after a change to the game. A simple
            pixel-by-pixel
            comparison of two screenshots is not sufficient, as some variations (e.g., because of character
            customization or
            weather conditions in the game) may be acceptable.
            Visual regressions may occur in cinematic parts of the game, such as cutscenes that have a defined sequence
            flow. We simulate this task by asking the VLM to compare whether two screenshots are similar, taking into
            account the specified acceptable or unacceptable variations.
        </p>
    </div>

    <div class="container blog extra-large gray">
        <img src="clarity/images/prismer.png" alt="Visual Regression Testing Sample">
        <p class="caption">
            Sample from the Visual Regression Testing task.
        </p>
    </div>

    <div class="container blog main">
        <h2>Glitch Detection</h2>
        <p>
            Glitch detection is the process of identifying unintended visual errors, such as rendering issues, clipping,
            or
            physics/logical bugs that express themselves visually. We simulate this task by asking the VLM whether
            glitch
            and glitch-free images contain a glitch.
        </p>
    </div>

    <div class="container blog extra-large gray">
        <img src="clarity/images/prismer.png" alt="Glitch Detection Sample">
        <p class="caption">
            Sample from the Glitch Detection task.
        </p>
    </div>

    <div class="container blog main">
        <h2>Parametric Clipping Detection</h2>
        <p>
            Given the common occurrence of clipping in games, our benchmark includes a dedicated task to evaluate a
            model's
            ability to detect such glitches. In this task, images feature an object (e.g., a cube, sphere, or character)
            positioned at varying distances from a human character â€” from far apart to fully overlapping/clipping. The
            VLM
            is asked whether it detects clipping across each of these distances.
        </p>
    </div>

    <div class="container blog extra-large gray">
        <img src="clarity/images/prismer.png" alt="Parametric Clipping Detection Sample">
        <p class="caption">
            Sample from the Parametric Clipping Detection task.
        </p>
    </div>

    <div class="container blog main">
        <h2>Bug Report Generation</h2>
        <p>
            In addition to testing and detection tasks, a potential application of VLMs is to assist QA engineers with
            writing reports for detected bugs. We simulate this task by asking the VLM to write a description of a
            glitch
            image that can be used in a bug report.
        </p>


    </div>

    <div class="container blog extra-large gray">
        <img src="clarity/images/prismer.png" alt="Bug Report Generation Sample">
        <p class="caption">
            Sample from the Bug Report Generation task.
        </p>
    </div>








    <div class="container blog main">
        <h1>Video-based Tasks</h1>
        <h2>Glitch Detection (Video)</h2>
        <p>
            Glitch detection in videos can be used to verify autonomous gameplay sessions from bots. Detecting glitches
            in
            videos is significantly more complex due to challenges such as analyzing motion patterns, and may require
            identifying transient glitches that appear only briefly in a few frames. We simulate this task by asking the
            vision-language model whether it detects a glitch in a video.
        </p>
    </div>

    <div class="container blog extra-large gray">
        <img src="clarity/images/prismer.png" alt="Bug Report Generation Sample">
        <p class="caption">
            Sample from the Glitch Detection (Video) task.
        </p>
    </div>

    <div class="container blog main">
        <h2>Needle-in-a-Haystack (NIAH)</h2>
        <p>
            Needle-in-a-Haystack (NIAH) is a more challenging long-context retrieval version of the glitch detection
            task.
            We simulate this task by asking the vision-language model whether it detects a glitch in a video, and in
            which
            frame the glitch occurs for the first time.
        </p>
    </div>

    <div class="container blog extra-large gray">
        <img src="clarity/images/prismer.png" alt="Bug Report Generation Sample">
        <p class="caption">
            Sample from the Needle-in-a-Haystack (NIAH) task.
        </p>
    </div>

    <div class="container blog main">
        <h2>Bug Report Generation (Video)</h2>
        <p>
            In this task, the vision-language model is asked to provide a description of a glitch video that can be used
            in
            a bug report.
        </p>
    </div>

    <div class="container blog extra-large gray">
        <img src="clarity/images/prismer.png" alt="Bug Report Generation Sample">
        <p class="caption">
            Sample from the Bug Report Generation (Video) task.
        </p>
    </div>





    <div class="container blog main">
        <h2>
            Leaderboard
        </h2>
        <p class="text">
            In Clarity, I have provided container widths with five options: <code>main</code>, <code>large</code>,
            <code>extra-large</code>, <code>extra-extra-large</code>, and <code>max</code>. These container layouts are
            designed to be responsive, automatically adjusting based on the screen size. The default width is
            <code>main</code>, which is used for this blog post. Be cautious when using the <code>max</code> option, as
            it has zero padding. Unless you are certain of the design, it's always recommended to leave some space for
            visual aesthetic purposes.
        </p>

        <p class="text">
            Here is an example I designed, illustrating the architectural details in <a
                href="https://shikun.io/projects/prismer">Prismer</a> using the <code>extra-large</code> container
            width.
        </p>
    </div>

    <div class="container blog extra-large gray">
        <img src="clarity/images/prismer.png">
        <p class="caption">
            Prismer has two main trainable components: the <b>Experts Resampler</b> that converts variable multi-task
            signals to a fixed number of outputs, and the <b>Adaptor</b> that enhances the model's expressivity for
            vision-language reasoning. To ensure that the model takes advantage of the rich domain-specific knowledge
            encoded in the pre-trained experts, the majority of network weights are frozen during training, as
            represented by the snowflake icon.
        </p>
    </div>

    <div class="container blog main">
        <h2>
            Container Colour
        </h2>

        <p class="text">
            In both visual diagram examples above, I used the <code>gray</code> option for the container to apply a
            subtle tint of gray, helping to distinguish it from the main container. This option is ideal for visual
            diagrams with a transparent background. For visual diagrams with a white background, I included the
            <code>gray-linear</code> option. This feature adds a tint of gray only to the edges of the container while
            keeping the center white, maintaining a clean and cohesive appearance while still providing distinct visual
            separation.
        </p>

        <p class="text">
            Here is an example illustrated in <a href="https://shikun.io/projects/variational-shape-learner">VSL</a>
            using the <code>gray-linear</code> container colour.
        </p>
    </div>

    <div class="container blog extra-large gray-linear">
        <p class="caption inline">
            <b>Inter-Class Shape Morphing</b>
        </p>
        <div class="columns-6">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_inter1.gif">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_inter2.gif">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_inter3.gif">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_inter4.gif">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_inter5.gif">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_inter6.gif">
        </div>
        <p class="caption inline">
            <b>Intra-Class Shape Morphing</b>
        </p>
        <div class="columns-6">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_intra1.gif">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_intra2.gif">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_intra3.gif">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_intra4.gif">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_intra5.gif">
            <img src="https://shikun.io/projects/vsl/shape_morph/morph_intra6.gif">
        </div>
        <p class="caption">Note that, although these animations have a white background, they beautifully blend within
            this container. </p>
    </div>

    <div class="container blog main">
        <h1>
            Design Features
        </h1>

        <p class="text">
            Here, I will briefly introduce each design feature included in the Clarity template.
        </p>

        <h2>
            Fundamentals
        </h2>

        <p class="text">
            In Clarity, I have selected <a href="https://fonts.google.com/specimen/Poppins">Poppins</a> as the main
            sans-serif font and <a href="https://practicaltypography.com/charter.html">Charter</a> as the main serif
            font. Both fonts are freely distributed and without license restrictions. (Note that these two fonts differ
            from the licensed fonts used on my personal website.) All font sizes and weights are carefully tuned to be
            visually balanced and responsive across various screen sizes.
        </p>

        <p class="text">
            Additionally, a simple grid system is provided in a separate CSS file. This can be easily applied by using a
            div with class <code>columns-x</code> (e.g., <code>columns-6</code> for creating the above VSL example).
            This CSS file is designed to be separate from the main CSS file, allowing for project-specific
            customisations on top of the broadly used features included in the Clarity template.
        </p>

        <h2>
            Mathematics
        </h2>

        <p class="text">
            Clarity supports $\LaTeX$ using the <a href="https://www.mathjax.org/">MathJax</a> JavaScript engine. It can
            elegantly render simple inline mathematical equations such as $y=ax+b$, as well as more complex displayed
            equations such as used in <a href="https://arxiv.org/abs/1312.6114">Variational Auto-Encoder (VAEs)</a>:
            $$
            L_{VAE}(x^{(i)}, \theta, \phi) = -\mathbb{E}_{z}\left[\log p_{\theta} \left(x^{(i)}|z\right) \right] +
            \text{KL}\left(q_\phi(z|x^{(i)}) \|\, p_\theta(z) \right).
            $$
        </p>

        <p class="text">
            I have chosen to use the <a href="https://ctan.org/pkg/tex-gyre-pagella?lang=en">Gyre-Pagella</a> font for
            $\LaTeX$, which blends well with the current serif font, ensuring a consistent and visually appealing
            presentation of mathematical content.
        </p>

        <h2>
            Code
        </h2>

        <p class="text">
            Clarity supports inline code blocks such as <code>python train.py</code>, as well as display code blocks.
            The display code blocks also support syntax highlighting using the <a
                href="https://highlightjs.org/">highlight.js</a> library. Here is an example using Classifier-Free
            Guidance with Rescale implementation proposed in <a href="https://arxiv.org/pdf/2305.08891">this paper</a>.

        <pre><code class="python">def apply_cfg(pos, neg, weight=7.5, rescale=0.7):
    # Apply regular classifier-free guidance.
    cfg = neg + weight * (pos - neg)

    # Calculate standard deviations.
    std_pos = pos.std([1,2,3], keepdim=True)
    std_cfg = cfg.std([1,2,3], keepdim=True)

    # Apply guidance rescale with fused operations.
    factor = std_pos / std_cfg
    factor = rescale * factor + (1 - rescale)
    return cfg * factor</code></pre>
        </p>

        <p class="text">
            The display code block can also used for bibliographies, such as:
        </p>

        <pre><code class="plaintext">@article{lecun2015deep,
    title={Deep learning},
    author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
    journal={nature},
    year={2015},
    publisher={Nature Publishing Group UK London}
}</code></pre>

        <p class="text">
            Clarity uses my personal favourite monospaced font: <a href="https://github.com/tonsky/FiraCode">Fira
                Code</a> in code blocks. It also supports rendering beautifully coding literatures such as
            <code>--> ++ != ## <<>></code>.
        </p>

        <h2>
            Tables
        </h2>

        <p class="text">
            Clarity provides table blocks where every odd row is re-coloured with a gray tint. This helps to easily
            distinguish information within different rows, particularly with very wide tables. Here is an example using
            different model variants proposed in <a href="https://arxiv.org/pdf/2302.13971">Llama-1</a>.

        <div class="table-wrapper">
            <table>
                <thead class="center">
                    <tr>
                        <th>Params</th>
                        <th>Dimension</th>
                        <th>$n$ heads</th>
                        <th>$n$ layers</th>
                        <th>Learning Rate</th>
                        <th>Batch Size</th>
                        <th>$n$ Tokens</th>
                    </tr>
                </thead>
                <tbody class="center">
                    <tr>
                        <td>6.7B</td>
                        <td>4096</td>
                        <td>32</td>
                        <td>32</td>
                        <td>$3e^{-4}$</td>
                        <td>4M</td>
                        <td>1T</td>
                    </tr>
                    <tr>
                        <td>13.0B</td>
                        <td>5120</td>
                        <td>40</td>
                        <td>40</td>
                        <td>$3e^{-4}$</td>
                        <td>4M</td>
                        <td>1T</td>
                    </tr>
                    <tr>
                        <td>32.5B</td>
                        <td>6656</td>
                        <td>52</td>
                        <td>60</td>
                        <td>$1.5e^{-4}$</td>
                        <td>4M</td>
                        <td>1T</td>
                    </tr>
                    <tr>
                        <td>65.2B</td>
                        <td>8192</td>
                        <td>64</td>
                        <td>80</td>
                        <td>$1.5e^{-4}$</td>
                        <td>4M</td>
                        <td>1T</td>
                    </tr>
                </tbody>
            </table>
        </div>
        </p>

        <h2>
            Slide Display
        </h2>

        <p class="text">
            The Slide Display feature is designed to allow readers to easily navigate and visualise multiple sequential
            or related visual elements, abstracted by a slide menu with dots. Here is an example using videos generated
            in <a href="https://nvlabs.github.io/stylegan3/">StyleGAN-3</a>.
        </p>
    </div>

    <div class="container blog main gray">
        <div class="slide-menu">
            <ul class="dots" id="slide-menu">
                <li class="dot active"></li>
                <li class="dot"></li>
                <li class="dot"></li>
            </ul>
        </div>

        <div class="slide-content" , style="display: block;">
            <video loop playsinline muted autoplay
                src="https://nvlabs-fi-cdn.nvidia.com/_web/stylegan3/videos/video_2_metfaces_interpolations.mp4"
                style="width: 100%"> </video>
        </div>

        <div class="slide-content" , style="display: none;">
            <video loop playsinline muted autoplay
                src="https://nvlabs-fi-cdn.nvidia.com/_web/stylegan3/videos/video_3_afhq_interpolations.mp4"
                style="width: 100%"> </video>
        </div>

        <div class="slide-content" , style="display: none;">
            <video loop playsinline muted autoplay
                src="https://nvlabs-fi-cdn.nvidia.com/_web/stylegan3/videos/video_4_beaches_interpolations.mp4"
                style="width: 100%"> </video>
        </div>


        <p class="caption">
            The videos show interpolations between hand-picked latent points in several datasets. Observe again how the
            textural detail appears fixed in the StyleGAN-2 result, but transforms smoothly with the rest of the scene
            in the alias-free StyleGAN-3.
        </p>
    </div>

    <div class="container blog main">
        <h2>
            Auto Slideshow Display
        </h2>
        <p class="text">
            Alternatively, the Auto Slideshow Display feature enables readers to enjoy a large array of visual elements
            through an automated, slideshow-like presentation. Users can also navigate through the slideshow manually
            with "Left" and "Right" buttons. Below is an example using videos generated in <a
                href="https://mardini-vidgen.github.io/">MarDini</a>.
        </p>
        <!-- 
        1. You may add another row of visual elements by simply adding a video element in each "slider-item" div. Correspondingly, you need to change the following in the scss file (under Slideshow Control): height: 440px into 440px * num_rows. 
        2. You may also change the fixed image size by changing the following:
            a. scss: .slider-item width: 400px -> changed width; 
            b. scss: height: 440px -> changed width + padding_bottom;
            c. javascript: item.style.left = `${(i-1) * 440}px`) -> changed width + padding_left
        -->
    </div>
    <div class="container blog max gray">
        <div class="slideshow">
            <div class="navigation">
                <!-- Using FontAwesome Pro -->
                <!-- <a class="button icon" id="prev_btn"><i class="fa-solid fa-left" ></i></a>
                <a class="button icon" id="next_btn"><i class="fa-solid fa-right"></i></a> -->
                <!-- Using FontAwesome Free -->
                <a class="button icon" id="prev_btn"><i class="fa-solid fa-arrow-left"></i></a>
                <a class="button icon" id="next_btn"><i class="fa-solid fa-arrow-right"></i></a>
            </div>
            <div class="slider">
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/1.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/2.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/3.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/4.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/5.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/6.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/7.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/8.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/9.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/10.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/11.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/12.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/13.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/14.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/15.mp4"></video>
                </div>
                <div class="slider-item">
                    <video loop playsinline muted autoplay
                        src="https://mardini-vidgen.github.io/clarity/videos/16.mp4"></video>
                </div>
            </div>
        </div>
    </div>

    <div class="container blog main">
        <h2>
            Selection Display
        </h2>

        <p class="text">
            The Selection Display feature is designed to visualise multiple visual elements based on a user-selectable
            menu. This feature is commonly used for images visualised with conditional prompts, allowing users to
            explore different visual outputs by selecting various conditions or prompts from the menu. Here is an
            example using images generated in <a href="https://sites.research.google/parti/">Google Parti</a>.
        </p>
    </div>

    <div class="container blog main gray">
        <div class="columns-2">
            <div>
                <img id="parti"
                    src="https://sites.research.google/parti/paper_images_green_watermark_outputs/figures/cherries/tornado/tiger_cubism.jpg">
                <p class="caption selection">
                    <b>Input Prompt: </b> <br>
                    A tornado made of
                    <Select id="image-selector1">
                        <option value="sharks" selected>sharks</option>
                        <option value="tiger">tigers</option>
                        <option value="bees">bees</option>
                    </Select>
                    <select id="image-selector1-temp" style="display: none;">
                        <option id="image-selector1-temp-option"></option>
                    </select>
                    crashing into a skyscraper. Painting in the style of
                    <Select id="image-selector2">
                        <option value="hokusai" selected>Hokusai</option>
                        <option value="cubism"> abstract cubism</option>
                        <option value="watercolor">watercolor</option>
                    </Select>
                    <select id="image-selector2-temp" style="display: none;">
                        <option id="image-selector2-temp-option"></option>
                    </select>
                    .
                </p>
            </div>

            <div>
                <img id="parti2"
                    src="https://sites.research.google/parti/paper_images_green_watermark_outputs/figures/cherries/cherry2/panda_tennis.jpg">
                <p class="caption selection">
                    <b>Input Prompt: </b> <br>
                    A photo of an Athenian vase with a painting of
                    <Select id="image-selector3">
                        <option value="panda" selected>pandas</option>
                        <option value="toucan">toucans</option>
                        <option value="pangolin">pangolins</option>
                    </Select>
                    <select id="image-selector3-temp" style="display: none;">
                        <option id="image-selector3-temp-option"></option>
                    </select>
                    playing
                    <Select id="image-selector4">
                        <option value="tennis" selected>tennis</option>
                        <option value="soccer"> soccer</option>
                        <option value="basketball">basketball</option>
                    </Select>
                    <select id="image-selector4-temp" style="display: none;">
                        <option id="image-selector4-temp-option"></option>
                    </select>
                    in the style of Egyptian hieroglyphics.
                </p>
            </div>
        </div>
    </div>

    <div class="container blog main">
        <h2>
            Comparison Display
        </h2>

        <p class="text">
            The Comparison Display feature allows users to visualise two images side by side with a controllable slider,
            making it easy to compare visual results against a baseline. This feature is particularly useful for
            showcasing improvements or differences between two images. Here is an example using upsampled images
            generated with <a href="https://mingukkang.github.io/GigaGAN/">GigaGAN</a>.
        </p>
    </div>
    <div class="container blog main gray extra-large">
        <div class="columns-2">
            <img-comparison-slider class="slider-container white">
                <figure slot="first" class="before">
                    <img src="https://mingukkang.github.io/GigaGAN/static/images/iguana_input.jpg">
                    <figcaption class="white">Input Image (128px)</figcaption>
                </figure>
                <figure slot="second" class="after">
                    <img src="https://mingukkang.github.io/GigaGAN/static/images/iguana_output.jpg">
                    <figcaption class="white">GigaGAN Upsampled (1024px)</figcaption>
                </figure>
            </img-comparison-slider>
            <img-comparison-slider class="slider-container">
                <figure slot="first" class="before">
                    <img src="https://mingukkang.github.io/GigaGAN/static/images/plant_input.jpg">
                    <figcaption>Input Image (128px)</figcaption>
                </figure>
                <figure slot="second" class="after">
                    <img src="https://mingukkang.github.io/GigaGAN/static/images/plant_output.jpg">
                    <figcaption>GigaGAN Upsampled (1024px)</figcaption>
                </figure>
            </img-comparison-slider>
        </div>

        <p class="caption">
            Our GigaGAN framework can also be used to train an efficient, higher-quality upsampler. This can be applied
            to real images, or to the outputs of other text-to-image models like diffusion. GigaGAN can synthesize ultra
            high-res images at 4k resolution in 3.66 seconds.
        </p>
    </div>

    <div class="container blog main">
        <h2>
            Video Control / Buttons
        </h2>

        <p class="text">
            Clarity supports video controls through a set of intuitive navigation buttons, making it easier for readers
            to pause, resume, and adjust playback speed as needed. This functionality is particularly useful for
            projects involving 3D vision and neural rendering. This feature was recently developed to support the <a
                href="https://kxhit.github.io/EscherNet">EscherNet</a> project, with an example demonstrating this
            control functionality below.
        </p>
    </div>


    <div class="container blog extra-large gray-linear">
        <div class="video-control">
            <a class="button" onclick="SlowVideo('eschernet')"><i class="fa-solid fa-backward"></i></a>
            <!-- Using FontAwesome Pro -->
            <!-- <a class="button"onclick="ToggleVideo('eschernet')"><i class="fa-solid fa-play-pause"></i></a> -->

            <!-- Using FontAwesome Free -->
            <a class="button" onclick="ToggleVideo('eschernet')"><i class="fa-solid fa-play"></i></a>

            <a class="button" onclick="FastVideo('eschernet')"><i class="fa-solid fa-forward"></i></a>
            <a class="button" onclick="RestartVideo('eschernet')"><i class="fa-solid fa-rotate-left"></i></a>
        </div>

        <div class="columns-6">
            <div>
                <p class="caption inline center">Zero-1-to-3-XL</p>
                <video loop playsinline muted autoplay class="eschernet-video"
                    src="https://kxhit.github.io/eschernet/GSO100/Zero123/output_GSO-xl/backpack/output.mp4">
                </video>
            </div>
            <div>
                <p class="caption inline center">EscherNet (1 View)</p>
                <video loop playsinline muted autoplay class="eschernet-video"
                    src="https://kxhit.github.io/eschernet/GSO100/EscherNet/N2M100/backpack/output.mp4">
                </video>
            </div>
            <div>
                <p class="caption inline center">EscherNet (2 Views)</p>
                <video loop playsinline muted autoplay class="eschernet-video"
                    src="https://kxhit.github.io/eschernet/GSO100/EscherNet/N2M100/backpack/output.mp4">
                </video>
            </div>
            <div>
                <p class="caption inline center">EscherNet (3 Views)</p>
                <video loop playsinline muted autoplay class="eschernet-video"
                    src="https://kxhit.github.io/eschernet/GSO100/EscherNet/N3M100/backpack/output.mp4">
                </video>
            </div>
            <div>
                <p class="caption inline center">EscherNet (5 Views)</p>
                <video loop playsinline muted autoplay class="eschernet-video"
                    src="https://kxhit.github.io/eschernet/GSO100/EscherNet/N5M100/backpack/output.mp4">
                </video>
            </div>
            <div>
                <p class="caption inline center">EscherNet (10 Views)</p>
                <video loop playsinline muted autoplay class="eschernet-video"
                    src="https://kxhit.github.io/eschernet/GSO100/EscherNet/N10M100/backpack/output.mp4">
                </video>
            </div>
        </div>

        <p class="video-speed" id="eschernet-msg">
            Speed: Ã—1.00
        </p>
    </div>

    <div class="container blog main">
        <h1>
            Other Design Tips
        </h1>

        <h2>
            Colour
        </h2>

        <p class="text">
            Colour is one of the most crucial visual elements in design and visualisation, and it is the main reason in
            making the <a href="https://www.theolympicdesign.com/olympic-games/the-look-of-the-games/paris-2024/">Paris
                Olympics 2024</a> such visually memorable and refreshing. While I'm not a colour expert, and there're
            always more to learn, I generally prefer using low-contrast, pastel-like colours in my designs. I typically
            use variants of gray and dark blue to represent common features, and employ warmer colours such as orange
            and red to highlight important design elements, as illustrated in the neural architecture design example
            from this blog post.
        </p>

        <p class="text">
            For a more detailed understanding of colour design, I highly recommend the <a
                href="https://m3.material.io/styles/color/system/overview">Material Design Colour Guidelines</a> and <a
                href="https://www.molecularecologist.com/2020/04/23/simple-tools-for-mastering-color-in-scientific-figures/">this
                blog post</a> â€” which provide valuable insights and are definitely worth exploring.
        </p>

        <h2>
            Icons
        </h2>
        <p class="text">
            In Clarity, all icons are integrated using <a href="https://fontawesome.com/">FontAwesome</a>. As one of the
            largest and most comprehensive icon libraries available, FontAwesome offers a vast collection of clean and
            minimalist icons that perfectly align with Clarity's design style and a wide range of use cases. I have
            found FontAwesome particularly valuable for many of my design projects, and its integration ensures that
            Clarity maintains a modern and consistent visual style throughout.

            <!-- I highly recommend purchasing the FontAwesome Pro version, as it offers a wider selection of icons and styles that integrate beautifully with this template. -->
        <h2>
            Data Plots and Visualisation
        </h2>

        <p class="text">
            I try to provide consistently high-quality plots useable for both $\LaTeX$ papers and websites using the <a
                href="https://www.overleaf.com/learn/latex/TikZ_package">TikZ</a> library. However, as many researchers
            know, TiKz offers stunning vector graphics but is notoriously difficult to use, and $\LaTeX$'s syntax design
            can be quite a headache (which makes new layout tools like <a href="https://typst.app/">Typst</a> quite
            intriguing).

        </p>

        <p class="text">
            To balance efficiency with quality, I typically use <a href="https://matplotlib.org/">matplotlib</a> and
            other Python tools to quickly prototype plots. I then use <a
                href="https://github.com/nschloe/tikzplotlib">tikzplotlib</a> to convert these plots into TiKz code.
            Finally, I refine the generated Tikz code for the final presentation. Although this process isn't always
            perfect, it has proven to be my most efficient solution, allowing me to create high-quality plots within a
            reasonable time (such as this example of raw TiKz code <a
                href="https://www.dropbox.com/scl/fi/fsiz2il9yoiymk94v55v8/vqav2.tex?rlkey=9yb86m8aar91a8tijcgf6pq3w&dl=0">here</a>).
        </p>
    </div>

    <div class="container blog extra-large">
        <div class="columns-2">
            <img src="https://shikun.io/projects/prismer/vqav2.png" style="width: 100%">
            <img src="https://shikun.io/projects/prismer/nocaps.png" style="width: 100%">
        </div>
    </div>

    <div class="container blog main">
        <h1>
            Open-Source Contribution and Future Plans
        </h1>

        <p class="text">
            The current version of Clarity already includes a range of design features suitable for many use cases.
            However, I plan to continue improving Clarity with new features and design elements as long as I am still
            working in AI research (and/or AGI is not solved :/). Since Clarity is open-source, I also welcome
            contributions from the community. Some example features that would be valuable additions include: 1. A
            content menu and hover citation system similar to those found in <a
                href="https://distill.pub/2021/understanding-gnns/">Distill blogs</a>, and 2. Music and sound layouts as
            demonstrated in <a href="https://openai.com/index/jukebox/">OpenAI Jukebox</a>.
        </p>

        <h1>
            License
        </h1>

        <p class="text">
            Following <a href="https://nerfies.github.io/">Nerfie</a>, arguably one of the most popular website
            templates currently available, Clarity is also licensed under the <a
                href="https://creativecommons.org/licenses/by-sa/4.0/"><b>Creative Commons Attribution-ShareAlike 4.0
                    International License</b></a>. This allows you to freely modify and distribute the template in any
            way you like. Similarly, I kindly ask that you link back to this page in the footer to acknowledge my work
            and efforts. Enjoy!
        </p>
    </div>


    <!-- Footer Page -->
    <footer>
        <div class="container">
            <p>
                This website is built on the <a href="https://shikun.io/projects/clarity">Clarity Template</a>, designed
                by <a href="https://shikun.io/">Shikun Liu</a>.
            </p>
        </div>
    </footer>


    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="clarity/clarity.js"></script>
    <script src="assets/scripts/main.js"></script>

</html>
</body>